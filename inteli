getEvaluationMetrics <- function(cm) {
  TP <- cm[2,2]
  TN <- cm[1,1]
  FP <- cm[1,2]
  FN <- cm[2,1]
  
  accuracy = sum(diag(cm))/sum(cm) # tacno predvidjene / sve
  precision <- TP / (TP + FP)      # tacno predvidjenje pozitivne / sve predvidjene pozitivne (prva kolona ili druga u zavisnosti od pozitivne klase)
  recall <- TP / (TP + FN)         # tacno predvidjenje pozitivne / prvi ili drugi red u zavisnosti od pozitivne klase
  F1 <- (2 * precision * recall) / (precision + recall)
  
  c(Accuracy = accuracy, 
    Precision = precision, 
    Recall = recall, 
    F1 = F1)
  
}

eval.tree1 <- getEvaluationMetrics(tree1.cm)
eval.tree1

# accuracy = procenat tacnih predikcija, ovde smo od ukupnog broja observacija
# u test setu, sto je 2163, tacno predvideli 2038, pa nam je tacnost visoka, 94.22%
# za 1579 smo rekli da nece da ima visok broj recenzija i pogodili, a za 459 da hoce i pogodili
# za 43 smo rekli da ce da imaju i pogresili, a za 82 da nece i pogresili

# precision = udeo onih koje smo predvideli da su pozitivne koje su stvarno pozitivne
# ovde smo od 502 observacije za koje smo rekli da imaju visok broj recenzija
# za 459 predvideli tacno, a za 43 smo rekli da imaju, a zapravo nemaju, pa nam
# je precision 91.43%

# recall = udeo observacija koje su stvarno pozitivne koje smo predvideli da su pozitivne
# ovde smo od ukupno 541 observacije sa visokim brojem recenzija 459 predvideli tacno,
# a za 82 smo rekli da nemaju visok broj recenzija, a zapravo imaju, pa nam 
# je recall nesto manji od precisiona, odnosno 84.84%

# F1 = sluzi za evaluaciju modela kada su precision i recall u balansu, 
# govori koliko je dobar model, u nasem slucaju je 88.01%, pa mozemo da 
# zakljucimo da jeste dobar
